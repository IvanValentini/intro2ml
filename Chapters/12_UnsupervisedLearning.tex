\chapter{Unsupervised Learning}

\section{Introduzione}
Nel unsupervised learning non si hanno label per guidare il learning algorithm, pertanto il meccanismo di learning sar\`a molto diverso.
Si osservano i dati da una distribuzione sconosciuta $p_{data}\in\Delta(X)$.
In quanto non si hanno osservazioni riguardo i target si deve introdurre un modello di conoscenza a priori o dare supervisione implicita attraverso il design della funzione obiettivo.

	\subsection{Task tipiche}
	Le task tipiche dell'unsupervised learning sono:
	\begin{itemize}
		\item Dimensionality reduction: incorporare i dati in uno spazio a meno dimensioni.
		\item Clustering: dividere i dati in gruppi con propriet\`a simili.
		\item Density estimation: imparare una distribuzione di probabilit\`a che fitta meglio il training data.
	\end{itemize}

\section{Dimensionality reduction}

	\subsection{Task}
	Si deve trovare una funzione $f\in Y^X$ che mappa ogni input a grandi dimensioni $x\in X$ a dimensioni minori incorporando $f(x)\in Y$, dove $dim(T)\ll dim(X)$.
	Questa task permette di preservare le informazioni riducendo la dimensione di ogni dato in input.
	Questo approccio permette di visualizzare i dati, semplificare la loro computazione.

	\subsection{Principal component anlaysis}
	In questo meccanismo di dimensionality reduction la conoscenza implicita messa nell'algoritmo \`e che la varianza \`e un indicatore di ricchezza di informazioni dati da una dimensione.
	PCA trova una trasformazione ottimale nel sistema di coordinate tale che perdere delle dimensioni nelle nuove coordinante porta la pi\`u piccola riduzione nella varianza dei dati.
	PCA in particolare  perde l'asse con la varianza minore perdendo meno informazioni possibile.

		\subsubsection{Varianza lungo una dimensione $\mathbf{w}$}
		Per calcolare la varianza lungo una data unit\`a di direzione $w$.
		$w$ \`e un'unit\`a di direzione se $w^Tw=1$.
		Sia $x_i$ un data point, si necessita di un punto nello spazio $c$ da cui si applica la direzione $w$ per calcolare la varianza di tutti i data points:
		$$t_i = (x_i-c)^Tw$$
		La proiezione di $x_i$ su $w$.
		Il valore atteso $\mathbb{E}[t]$:
		\begin{align*}
			\mathbb{E}[t] &= \frac{1}{n}\sum\limits_{i=1}^nt_i = \frac{1}{n}\sum\limits_{i=1}^n(x_i-c)^Tw=\\
			&\bar{x}^Tw-c^Tw
		\end{align*}
		Ora la varianza sulla dimensione definita da $w$ \`e:
		\begin{align*}
			Var[t] &= \frac{1}{n}\sum\limits_{i = 1}^n(t_i -\mathbb{E}[t])^2 = \frac{1}{n}\sum\limits_{i=1}^n[(x+i-\bar{x})^Tw]^2=\\
						 &=\frac{1}{n}\sum\limits_{i=1}^n(\bar{x}_i^Tw)^2 = w^T\bigl[\frac{1}{n}\bar{X}\bar{X}^T\bigr]w=\\
						 &=w^TCw
		\end{align*}
		Dove $C$ \`e la matrice della covarianza e $X = [\bar{x}_1,\dots,\bar{x}_n]$.
		Si nota come la computazione della varianza non dipende dal punto $c$ in quanto \`e implicitamente calcolata da un punto di vista centrato.

		\subsubsection{Eigenvalue decomposition}
		Sia $A\in R^{m\times m}$ quadrata e simmetrica.
		Allora esiste $U=[u_1,\dots,u_n]\in R^{m\times m}$ e $\lambda=(\lambda_1,\dots,\lambda_m)^T\in R^m$ tali che:
		$$A = U\Lambda U^T=\sum\limits_{j=1}^m\lambda_ju_ju_j^T$$
		E $U^TU=UU^T=I$ e $U$ \`e ortonormale.
		Ogni colonna di $U$ ha lunghezza di unit\`a e ogni paio di colonne diverse sono ortogonali tra di loro.
		La matrice $\Lambda$ \`e creata con zero da tutte le parti e $\lambda$ sulla diagonale.
		$u_j$ \`e un eigenvector e $\lambda_j$ \`e la eigenvalue corrispondente.
		Si assume un ordinamento discendente: $\lambda_1\ge \lambda_m$.

		\subsubsection{First principal component}
		Il first principal component \`e la direzione dove la maggior parte della varianza \`e conservata.
		Si deve risolvere il problema di massimizzazione della varianza:
		$$w_1\in arg\max\{w^TCw:w^Tw=1\}$$
		Si devono fare costraints su $w_1$ altrimenti crescerebbero all'infinito.
		La varianza di PCA \`e il valore del pi\`u grande eigenvalue della matrice $C$ di covarianza, la prima componente principale \`e il corrispondente vettore $w_1$.
		Il maggiore eigenvalue di $C$ \`e la varianza lungo il first principal component e il first principal component $w_1$ \`e il corrispondente eigenvector.

			\paragraph{Dimostrazione}
			Dalla decomposizione di eigenvalue $C=\sum\limits_j\lambda_ju_ju_j^T$ si assumono le eigenvalue ordinate in ordine discendente.
			Allora $W_1^TCw_1 = \sum\limits_j\lambda_j(w_1^Tu_j)^2\le\lambda_1$ in quanto:
			$$\sum\limits_j(w_1^Tu_j)^2 = w_1^T\sum\limits_ju_ju_j^Tw_1=w_1^TUU^Tw_1=w_1^Tw_1=1$$
			Segue che $\lambda_1\ge w_1^TCw_1 >u_1^TCu_1=\lambda_1$, da cui $w_1^TCw_1=u_1^TCu_1$.
			Pertanto $u_1$, l'eigenvector corrispondente al eigenvalue maggiore $\lambda_1$ di $C$ \`e il first principal componente e $\lambda_1$ la varianza lungo di esso.

		\subsubsection{Second principal component}
		Il second principal component deve essere ortogonale al primo:
		$$w_2\in arg\{w^TCw:w^Tw=1,w\perp w_1\}$$
		Il secondo eigenvalue maggiore di $C$ \`e la varianza lungo il second principal component e $w_2$ \`e il corrispondente eigenvector.

			\paragraph{Dimostrazione}
			Dalla decomposizione di eigenvalue $C=\sum\limits_j\lambda_ju_ju_j^T$ si assumono le eigenvalue ordinate in ordine discendente.
			Allora
			$$W_2^TCw_2 = \sum\limits_{j=1}^m\lambda_j(w_2^Tu_j)^2=\lambda_1w_2^Tu_1+\sum\limits_{j=2}^m\lambda_j(w_2^Tu_j)^2\le\lambda_2$$
			In quanto:
			$$\sum\limits_j(w_2^Tu_j)^2 = w_2^T\sum\limits_ju_ju_j^Tw_2=w_2^TUU^Tw_2=w_2^Tw_2=1$$
			Segue che $\lambda_2\ge w_2^TCw_2 >u_2^TCu_2=\lambda_2$, da cui $w_2^TCw_2=u_2^TCu_2$.
			Pertanto $u_2$, l'eigenvector corrispondente al secondo eigenvalue maggiore $\lambda_2$ di $C$ \`e il second principal componente e $\lambda_2$ la varianza lungo di esso.

		\subsubsection{Iesimo principal component}
		Allo stesso modo:
		$$w_i\in arg\{w^TCw:w^Tw=1,w\perp w_j 1\le j < i\}$$
		L'i-esimo eigenvalue maggiore di $C$ \`e la varianza lungo l'iesimo principal component.
		L'iesimo principal component \`e $w_i$ \`e il corrispondente eigenvector.
		La dimostrazione \`e analoga a quella del second principal component.

		\subsubsection{PCA utilizzando eigenvalue decomposition}
		\begin{itemize}
			\item Siano i data points $X=[x_1,\dots,x_n]$.
			\item Si centri $\bar{X} = X-\frac{1}{n}X1_n1_n^T$.
			\item Si computi la matrice di covarianza $C=\frac{1}{n}\bar{X}\bar{X}^T$.
			\item Eigenvalue decomposition: $U,\lambda = eig(C)$.
			\item Principal components: $W=U=[u_1,\dots,u_m]$, varianze $\lambda = (\lambda_1,\dots,\lambda_m)$.
		\end{itemize}

		\subsubsection{PCA utilizzando singular value decomposition}
		Sia $A\in\mathbb{R}^{m\times n}$.
		Allora esiste $U\in\mathbb{R}^{m\times k}$, $s\in\mathbb{R}^k$ con $s_1\ge\cdots\ge s_K >0$ e $V\in \mathbb{R}^{n\times k}$ tali che:
		$$A = USV^T\qquad\qquad\land\qquad\qquad U^TU=V^TV=I$$
		Si computa pertanto la SVD di $\bar{X}:U,s,V=SVD(\bar{X})$.
		Le componenti principali $U=[u_1,\dots,u_k]$ e le varianze $\bigl(\frac{s_1^2}{n},\dots,\frac{s_k^2}{n}\bigr)$
		La matrice $U$ \`e la matrice con gli eigenvectors.
		Le eigenvalue possono essere calcolate in quanto: $\bar{x} = USV^T$ e $c = \frac{1}{n}\bar{x}\bar{x}^T = \frac{1}{n}USV^TVSU^T=U\frac{s^2}{n}u^T$

		\subsubsection{Dimensionality reduction}
		Sia $\hat{W} = [w_1,\dots,w_k]$ le prime $k$ componenti principali derivate dai data points $\bar{X} = [\bar{x}_1,\dots,\bar{x}_n]$.
		Si cambia a un sistema di coordinate ridotto con le $k$ componenti principali con $\hat{W}$ come assi:
		$$T = \hat{W}^T\bar{X}\in\mathbb{R}^{k\times n}$$
		Dove $T$ \`e il principal component scores.
		Si pu\`o usare la eigenvalue decomposition o SVD per computare la decomposizione completa.
		Con la PCA si ottengono gli eigenvectors o componenti, si possono ordinare e usarli per comporre la matrice $\hat{W}$.
		Una volta fatto quello si usa per calcolare $T$ che \`e il dataset dimensionalmente ridotto.

		\subsubsection{Interpretazioni alternative}
		Si pu\`o considerare la first principal component come la linea nello spazio con la minore distanza quadrata dai data points.
		La stessa interpretazione pu\`o essere data alle altre componenti principali.

		\subsubsection{Scalare delle variabili}
		PCA \`e sensibile alla scala delle features, pertanto \`e raccomandato scalarle secondo la standard deviation.

		\subsubsection{Componenti principali da considerare}
		Il numero di componenti per la dimensionality reduction dipende dall'obiettivo e dall'applicazione.
		Non ci sono modi per validarla a meno di volerla usare in un contesto di un modello con supervision, ma si pu\`o calcolare la proporzione cumulativa di varianza spiegata che per i primi principal component \`e:
		$$\frac{\sum\limits_{j = 1}^k\lambda_j}{\sum\limits_{j = 1}^m C_{jj}}$$
		Per la eigenvalue decomposition e
		$$\frac{\sum\limits_{j = 1}^ks^2_j}{\sum\limits_{ij} \bar{X}^2C_{ji}}$$
		Queste formule permettono di stimare la quantit\`a di informazione persa calcolando la percentuale di varianza che si \`e mantenuta riducendo la dimensionalit\`a dei dati.

		\subsubsection{Kernel PCA}
		La PCA riduce la dimensionalit\`a attraverso una trasformazione lineare.
		Pertanto utilizzando il kernel trick si pu\`o applicare una PCA in uno spazio a pi\`u dimensioni ottenendo una trasformazione non lineare nello spazio originale.

\section{Clustering}

	\subsection{Task}
	Si deve trovare una funzione $f\in N^X$ che assegna ogni input $x\in X$ a un indice di cluster $f(x)\in N$.
	Tutti i punti mappati allo stesso indice formano un cluster.
	Ci sono diversi tipi di cluster: partizionali, gerarchici e overlapped.
	Permette di trovare gruppi di dati con delle propriet\`a interne, comprimere i dati riducendo il numero di data points invece di ridurre la dimensionalit\`a delle features.

	\subsection{K-means clustering}
	Il K-means clustering richiede di sapere prima il numero $k$ di gruppi in cui si vogliono dividere i dati.
	Siano i data points $X=[x_1,\dots,x_n]\in\mathbb{R}^{d\times n}$.
	Fissato un numero di cluster $k$, si vuole trovare una partizione di data points in $k$ insiemi $\mathcal{L}_1,\dots,\mathcal{L}_k$ che minimizza la variazione $V(\mathcal{L}_j)$ in ogni set $\mathcal{L}_j$
	$$\min\limits_{\mathcal{L}_1,\dots,\mathcal{L}_K}\sum\limits_{j = 1}^kV(\mathcal{L}_j)$$
	La variazione \`e tipicamente data da $V(\mathcal{L}_j) = \sum\limits_{i\in\mathcal{L}_j}||x_i-\mu_j||^2$, dove $\mu_j = \frac{1}{|\mathcal{L}_j|}\sum\limits_{i\in\mathcal{L}_j}x_i$, il centroide di $\mathcal{L}_j$.
	Si deve definire una funzione obiettivo che minimizza la somma di variazioni in ogni set creato.
	L'algoritmo di ottimizzazione \`e semplice, inizializza con centroidi casuali e poi mentre i cluster cambiano assegna ogni datapoint al centroide pi\`u vicino formando nuovi cluster e computando nuovi centroidi.
	Questo algoritmo ha una convergenza garantita in quanto migliora strettamente la assignment di cluster e siccome sono finiti a un certo punto converge per forza.
	Non \`e comunque garantito trovi il minimo in quanto \`e un problema NP-hard anche sul piano.
	\`E sensibile alla scala delle features.
\section{Density estimation}

	\subsection{Task}
	Si deve trovare una distribuzione di probabilit\`a $f\in \Delta(X)$ che fitta i dati $x\in X$.
	Permette di ritornare una stima esplicita della distribuzione di probabilit\`a che genera i dati.
	Permette la generazione di nuovi dati dalla stessa distribuzione e l'individuazione di anomalie.

	\subsection{Generative model}
	I generative model risolvono la task della density estimation.

		\subsubsection{Modelli generativi e discriminativi}
		I modelli generativi sono modelli statistici della distribuzione dei dati sull'input $p_x$ o di una joint distribution sulla coppia di input label $p_{XY}$.
		La loro abilit\`a principale \`e di generare nuovi dati dalle distribuzioni osservate.
		I modelli discriminativi incece sono modelli statistici della distribuzione di probabilit\`a condizionale $p_{Y|X}$ del target dato l'obiettivo.
		Tipicamente svolgono una task di classificazione come SVM, decision trees e classificatori KNN.
		Un modello discriminativo pu\`o essere costruito da uno generativo attraverso la regola di Bayes ma non viceversa:
		$$p_{Y|X}(x) = \dfrac{p_{XY}(x,y)}{\sum\limits_{y'}p_{XY}(x,y')p_X(x)}$$

		\subsubsection{Tipi di density estimation}
		Per entrambi i modi si trovano due tipi della task:
		\begin{itemize}
			\item Supervised: $Z\in X\times Y$.
			\item Unsupervised $Z\in X$.
		\end{itemize}

			\paragraph{Explicit density estimation}
			Si deve trovare una distribuzione di probabilit\`a $f\in \Delta(Z)$ che fitta i dati $z\in Z$ dove $z$ \`e campionato da una distribuzione di dati sconosciuta $p_{data}\in\Delta(Z)$.

			\paragraph{Implicit density estimation}
			Si vuole trovare una funzione $f\in Z^\Omega$ che genera i dati $f(\omega)\in Z$ da un input $\omega$ campionato da una distribuzione predefinita $p_\omega\in\Delta(\Omega)$ in modo che la distribuzione del campione generato fitta la distribuzione sconosciuta $p_{data}\in\Delta(Z)$.
			Non si stima la probabilit\`a $\omega$ ma ci si concentra unicamente sul riprodurre i dati con la stessa distribuzione di quella sconosciuta originaria.

		\subsubsection{Obiettivo per i modelli generativi}
		Si deve definire uno spazio di ipotesi $H\subset \Delta(Z)$, consiste di un insieme di distribuzioni di probabilit\`a che possono essere definite esplicitamente o implicitamente.
		Si definisce una misura di divergenza $d\in R_+^{\Delta(Z)\times\Delta(Z)}$ tra le distribuzione di probabilit\`a in $\Delta(Z)$ e si usa la divergenza di Kullback-Leibler.
		La divergenza \`e $0$ se due distribuzioni hanno un match, altrimenti \`e negativa.
		L'algoritmo tenta di trovare $q^*\in H$ che ha un fit migliore sui dati distribuiti secondo $p_{data}$ dove il best fit \`e quello con la divergenza minore.
		$$q^*\in arg\min\limits_{q\in\mathcal{H}}d(p_{data},q)$$
		Si assumer\`a che la distribuzione dei dati \`e solo su $X$ con unsupervised learning, ma il trasporto a supervised \`e banale.

		\subsubsection{Variational AutoEncode (VAE)}
		Un autoencoder \`e un modo per comprimere dati ad alta dimensione in una rappresentazione a meno dimensioni: dimensionality reduction.
		Un encoder mappa i dati di input $x$ a una rappresentazione compressa $\omega$.
		La rappresentazione conserva fattori significativi nella variazione dei dati come in PCA.
		Un encoder viene trainato creando un decoder che mappa la rappresentazione di $\omega$ indietro nel dominio di input portando a una ricostruzione $\hat{x}$.
		Pertanto l'encoder autoencoda il proprio input.
		L'obiettivo \`e quello di minimizzare la divergenza tra l'input $x$ e la sua ricostruzione $\hat{x}$ che porta all'algoritmo di training verso la minima perdita di informazione durante la fase di encoding.
		Dopo il training il decoder non \`e pi\`u necessario in quanto \`e funzionale solo per stimare l'encoder.
		L'encoder invece pu\`o essere utile per diverse tasks, per esempio per inizializzare o precomputare le caratteristiche per supervised models.
		Il decoder potrebbe essere usato per generare nuovi dati ma non li generer\`a secondo $p_{data}$ in quanto niente lo obbliga.
		Se si genera un input a caso $\omega$ non si \`e sicuri che $\omega$ \`e una combinazione che verrebbe ordinata dalla distribuzione dei dati.
		La soluzione a questo problema sarebbe di avere una distribuzione a priori $\Omega$ che ci dice la probabilit\`a di campionare ogni $\omega$ dallo spazio encodato.
		Il decoder poi sarebbe capace di tradurre la distribuzione $\Omega$ nella distribuzione di dati in $X$.
		In termini formali il decoder: $q_\theta(x|\omega)$ \`e una distribuzione di probabilit\`a $x$ per ogni valore $\omega$.
		La probabilit\`a a priori \`e $p_\omega$
		Si pu\`o ottenere un'aspettativa marginalizzata
		$$q_\theta(x) = \mathbb{E}_{\omega\sim p_\omega}[q_\theta(x|\omega)]$$
		Successivamente si pone come obiettivo modificare il parametro $\theta$ per minimizzare la divergenza tra la distribuzione dei dati e $q_\theta$:
		$$\theta^*\in arg\min\limits_{\theta\in\Omega}d(q_\theta,p_{data})$$
		Si utilizza KL-divergence.
		\`E non negativa e $0$ se $p$ e $q$ sono la stessa distribuzione.
		$$d_{KL}(p,q) = \mathbb{E}_{x\sim p}\bigl[\log\frac{p(x)}{q(x)}\bigr]$$
		Analizzando la divergenza si vede che il suo calcolo \`e intrattabile a causa del valore atteso.
		Si pu\`o approssimare il valore atteso attraverso stochastic gradient descent.
		Queste stime sono comunque dipendenti da $\omega$ e questo le rende grandemente biased.
		Si introduce pertanto un nuovo termine $q_\phi(x)\in\Delta(\Omega)$.
		Viene usato per calcolare due termini: un recostruction term e un regolarizzatore.
		Questi due possono essere usati per calcolare stime del gradiente non biased con rispetto a $\theta$ e $\omega$.
		Se si assume che il encoder e il decoder sono modellati per ritornare dati con la probabilit\`a modellata gaussiana (???? inserire)

		\subsubsection{VAE condizionale}
		Si assuma di avere un informazione $y\in\mathcal{Y}$ e si vuole generare nuovi dati sull'infomrazione.
		Si modifica il encoder e il decoder per prendere le informazioni in input ottenendo
		$$q_\phi(\omega|x,y)$$
		$$q_\theta(\omega|x,y)$$
		Si definisce i priori condizionati sull'informazione $p_\omega(\omega|y)$.
		Le VAE condizioneli sono usate per direzionare l'output che si vuole dal generative model, altrimenti l'output seguirebbe solo la distribuzione di probabilit\`a dei dati.

		\subsubsection{Probabilit\`a dei VAE}
		\begin{itemize}
		\item Underfitting: agli stage iniziali il regolarizzatore \`e troppo forte e tende ad annullare la capacit\`a del modello.
		\item Blurry samples: il generatore tende a produrre data blurry in quanto la gaussiana tende a produrre blurry risultati.
	\end{itemize}

	\subsection{Generative adversarial Networks (GAN)}
	Le GAN permettono di stimare la densit\`a implicitamente.
	SI assuma di avere una densit\`a a priori $p_\omega\in\Delta(\Omega)$ e un generatore o decoder $g_\theta\in X^\omega$ che genera i data points in $X$ dato un random point da $\Omega$.
	Il generatore campiona direttametne dalla distribuzione implicita che non si conosce.
	La densit\`a \`e indotta da $p_\omega$ e il generatore $g_\theta$ \`e dato da $q_\theta(x) = \mathbb{E}_{\omega\sim p_\omega}\delta[g_\theta(\omega) - x]$ dove $\delta$ \`e la funzione delta di Dirac.
	L'obiettivo di GAN \`e trovare $\theta^*$ tale che $q_{\theta^*}$ che fitta meglio la distribuzione $p_{data}$ sotto la divergenza di Jensen-Shannon $d_{jS}$:
	$$\theta^*\in arg\min\limits_{\theta} d_{JS}(p_{data},q_\theta)$$
	Dove:
	$$d_{JS} = \frac{1}{2}d_{KL}\bigl(p,\frac{p+q}{w}\bigr)+\frac{1}{2}d_{KL}\bigl(q,\frac{p+q}{2}\bigr)$$
	Che \`e chiaramente intrattabile da computare in quanto richiama la convergenza $KL$.
	Con dei passaggi matematici si arriva a una forma equivalent che porta a un problema di minimizzazione nella forma:
	$$\theta^*\in arg\min\limits_\theta\{\mathbb{E}_{x\sim p_{data}}[\log t_\phi(x)] + \mathbb{E}_{x\sim q_\theta}[\log(1-t_\theta(x))]\}$$
		In cui $t_\phi(x)$ \`e un classificatore detto discriminatore per i data point in $X$ che predice se un data point $x$ viene da $p$ o $q$.
		Si pu\`o imparare $t_\phi(x)$  per risolvere il problema di minmax.
		Anche in questa forma \`e intrattabile in quanto dipende dalla densit\`a specifica di $q_\theta$ ma si pu\`o sostituire con $g_\theta$:
		$$\theta^*\in arg\min\limits_{\theta}\{\mathbb{E}_{x\sim p_{data}}[\log t_\phi(x)]+\mathbb{E}_{\omega\sim p_\omega}[\log(1-t_\phi(g_\theta(\omega)))]\}$$

	\subsection{Game theoretic interpretation}
	Questo pu\`o essere visto come un gioco a due giocatori in cui il giocatore 1 o il generatore tenta di generare i dati che non possono essere distinte dai dati veri.
	Il giocatore 2 \`e il discriminatore che cerca di indovinare se l'input viene dalla vera distribuzione o \`e falso.
	Il payoff \`e preso con segno positivo per il discriminatore e negativo per il generatore.
	Pertanto \`e descritto come un gioco non cooperativo a due giocatori e zero somma.
	Il payoff \`e pertanto:
	$$V(\theta,\phi) = \mathbb{E}_{x\sim p_{data}}[\log t_\phi(x)]+\mathbb{E}_{x\sim p_\omega}[\log(1-t_\phi(g_\theta(\omega)))]$$
	Si pu`\o pertanto scrivere il problema di minmax come:
	$$\theta^*\in V(\theta,\phi)$$
	IL gradiente del obiettivo del GAN rispetto ai parametri del generatore $\theta$ $\frac{d}{d\theta}V(\theta,\phi)$ e richiede la risoluzione del problema di massimizzazione con rispetto al parametro del discriminatore $\phi$.
	Questo dovrebbe essere proibitivo computazionalmente.
	IN pratica si alterna uno step di update per il generatore dopo $k$ update per il discriminatore, senza garanzia di convergenza.

	\subsection{Problemi con GAN}
	\begin{itemize}
		\item Training stability: i parametri oscillano e non convergono.
		\item Mode collapse: il generatore potrebbe imparare a perfezionare pochi esempi dal training set e riprodurre solo quegli esempi per vincere sempre.
		\item Vanishing gradient: se il discriminatore \`e molto brao e lascia il generatore con poco gradiente per imparare.
	\end{itemize}
