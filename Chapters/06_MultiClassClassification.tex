\chapter{Multi class classification}

\section{Introduzione}

	\subsection{Classificazione binaria}
	Si \`e definita la classificazione binaria come la task in cui dati:
	\begin{multicols}{2}
		\begin{itemize}
			\item Uno spazio di input $\mathcal{X}$.
			\item Una distribuzione sconosciuta $\mathcal{D}$ su $\mathcal{X}\times\{-1,+1\}$.
			\item Un training set $D$ campionato da $\mathcal{D}$
		\end{itemize}
	\end{multicols}
	Si deve computare una funzione $f$ che minimizza $\mathbb{E}_{(x,y)\sim\mathcal{D}}[f(x)\neq y]$

	\subsection{Classificazione multi classe}
	La classificazione multiclasse \`e l'estensione naturale della classificazione binaria.
	L'obiettivo \`e quello di assegnare una label discreta a degli esempi.
	La differenza \`e che ora ci sono $k>2$ classi da cui scegliere.
	Dati pertanto:
	\begin{multicols}{2}
		\begin{itemize}
			\item Uno spazio di input $\mathcal{X}$ e un numero di classi $K$.
			\item Una distribuzione sconosciuta di $\mathcal{D}$ su $\mathcal{X}\times[K]$.
			\item Un training set $D$ campionato da $\mathcal{D}$.
		\end{itemize}
	\end{multicols}
	Si deve computare una funzione $f$ che minimizza $\mathbb{E}_{(x,y)\sim\mathcal{D}}[f(x)\neq y]$.

		\subsubsection{K nearest neighbours}
		Si noti come una \emph{K-NN} per classificare un esempio $d$ trova i $k$ vicini di $d$ e sceglie la label in presenza maggiore tra i $k$ vicini pi\`u prossimi.
		Non necessita pertanto di cambi algoritmici nel caso della multi class classification.

		\subsubsection{Decision tree}
		I decision tree non richiedono cambi algoritmici per la multi class classification.

	\subsection{Approccio black box alla multi class classification}
	Dato un classificatore binario questo si pu\`o usare per risolvere il problema della multiclass classification.
	Si noti come un perceptron oltre al risultato pu\`o anche dare un punteggio di confidenza.
	Inoltre siccome una linea non \`e sufficiente per dividere le classi se ne possono usare diverse.

\section{One versus all \emph{OVA}}
Nell'approccio \emph{OVA} nel training si definisce per ogni label $L$ un problema binario in cui:
\begin{multicols}{2}
	\begin{itemize}
		\item Tutti gli esempi con la label $L$ sono positivi.
		\item Tutti gli altri esempi sono negativi.
	\end{itemize}
\end{multicols}
In pratica si imparano $L$ diversi modelli di classificazione.
Si ricordi come il classificatore divide il piano in due semipiani.

	\subsection{Ambiguit\`a}
	In questo caso si formano pertanto delle zone in cui si creano delle ambiguit\`a.
	Se il classificatore non fornisce confidence e c'\`e ambiguit\`a si sceglie una delle label in conflitto.
	Nella maggior parte dei casi i classificatori forniscono confidence, allora in questo caso si:
	\begin{multicols}{2}
		\begin{itemize}
			\item Si sceglie il positivo con confidence maggiore.
			\item Se nessuno \`e positivo si sceglie il negativo con confidence minore.
		\end{itemize}
	\end{multicols}
	La confidence nel perceptron si calcola come distanza dall'iperpiano stabilito dalla prediction.

	\subsection{Algoritmi}
	\input{Pseudocodice/03_oneversusalltrain}
	\input{Pseudocodice/04_oneversusalltest}

\section{All versus all \emph{AVA}}
Un approccio alternativo consiste nel gestire il problema della classificazione multi classe decomponendolo in problemi di classificazione binaria.
Questo approccio viene detto anche all pairs.
Si classificano $\frac{K(K-1)}{2}$ classificatori in modo che
$$F_{ij}, 1\le i< j \le K$$
Sia il classificatore che discrimina la classe $i$ contro la classe $j$.
Questo classificatore riceve tutti gli esempi della classe $i$ come positivi e tutti gli esempi della classe $j$ come negativi.
Quando arriva un punto di test si valuta su tutti i classificatori $F_{ij}$.
Ogni volta che $F_{ij}$ predice positivo la classe $i$ prende un voto, altrimenti lo prende $j$.
Dopo aver eseguito tutti i $\frac{K(K-1)}{2}$ classificatori la classe con pi\`u voti decide la label.

	\subsection{\emph{AVA} training}
	Per ogni coppia di label si addestra un classificatore che le distingue:
	\input{Pseudocodice/05_allversusalltrainingpseudo}

	\subsection{\emph{AVA} classification}
	Per classificare un esempio $x$ lo si classifica per ogni classificatore $F_{ij}$.
	Per scegliere la classe finale si pu\`o:
	\begin{multicols}{2}
		\begin{itemize}
			\item Considerare la maggioranza.
			\item Considerare un voto pesato basato sulla confidence:
				\begin{itemize}
					\item $y = F_{ij}(x)$.
					\item $score_j +=y$.
					\item $score_i -=y$.
				\end{itemize}
				Lo score viene cambiato in quanto se $y$ \`e positivo il classificatore lo pensa di tipo $j$, se negativo lo pensa di tipo $i$ e pertanto lo score viene aggiornato di conseguenza.
		\end{itemize}
	\end{multicols}

	\subsection{Algoritmi}
	\input{Pseudocodice/06_allversusalltrain}
	\input{Pseudocodice/07_allversusalltest}

\section{Confronto tra \emph{OVA} e \emph{AVA}}

	\subsection{Tempo di training}
	\emph{AVA} impara pi\`u classificatore ma il training set \`e omlto pi\`u piccolo pertanto tende ad essere pi\`u veloce se le label sono equamente bilanciate.

	\subsection{Tempo di test}
	Avendo \emph{AVA} pi\`u classificatori \`e tipicamente pi\`u lenta.

	\subsection{Errori}
	\emph{AVA} fa training con data sets pi\`u bilanciata, ma avendo pi\`u classificatori i test tendono ad avere pi\`u possiblit\`a di errori.

\section{Riassunto}
Se vengono usati classificatori binari viene tipicamente utilizzata  \emph{OVA}, altrimenti si usa un classificatore che permette label multiple come \emph{DT} o \emph{K-NN}, nonostante altri metodi pi\`u sofisticati siano meglio.

\section{Multiclass evaluation}

	\subsection{Microaveraging}
	Nel microaveraging si fa la media sugli esempi.

	\subsection{Macroaveraging}
	Nel macroaveraging si calcola lo score di valutazione o accuratezza per ogni label e poi si fa la media tra le label.
	Questo in quanto d\`a pi\`u enfasi a label pi\`u rare e permette un'altra dimensione di analisi.

	\subsection{Confusion matrix}
	La confusion matrix \`e una matrice in cui $(i, j)$ rappresenta il numero di esempi con label $i$ predetti avere label $j$.
	Viene spesso espressa come percentuale.
